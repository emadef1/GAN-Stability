{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3ChX6DMh5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "# from utils.const import *\n",
        "# from utils.helperFunctions import *\n",
        "# from utils.models import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path of the model (saved/to save)\n",
        "modelFolder = './models/'\n",
        "\n",
        "# When True, retrain the whole model\n",
        "retrain = True\n",
        "\n",
        "# Downsample the dataset\n",
        "ds = True\n",
        "\n",
        "# Size of the split\n",
        "trainSize = 0.75\n",
        "valSize = 0.05\n",
        "testSize = 0.20\n",
        "\n",
        "# Specify number of seconds for the window. Default: 16\n",
        "window_size = 16\n",
        "\n",
        "# Model hyper-parameters\n",
        "batch_size = 4\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Seed for reproducibility\n",
        "seed = 151836\n",
        "\n",
        "# Classes to drop in the dataset\n",
        "classes_to_drop=[\n",
        "    'stabf','stab']\n",
        "\n"
      ],
      "metadata": {
        "id": "5uHnLGRUgLDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "def setSeed(seed=seed):\n",
        "    \"\"\"\n",
        "    Setting the seed for reproducibility\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "setSeed()\n",
        "\n",
        "def min_max_norm(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n",
        "\n",
        "\n",
        "def std_scaler(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n",
        "\n",
        "\n",
        "def f1(test_loader, model):\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            outputs = model(data)\n",
        "            pred = outputs.data.max(1, keepdim=True)[1]\n",
        "            f1 += f1_score(labels, pred, average='macro')\n",
        "    avg_f1 = f1/len(test_loader)\n",
        "    return (avg_f1)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path='/content/new_dataset.csv', classes_to_drop=classes_to_drop, window_size=window_size, normalize=True, normalize_method='mean_std', auth=False, target=None):\n",
        "\n",
        "        self._window_size=window_size\n",
        "        self._data=pd.read_csv(file_path)\n",
        "\n",
        "        # if auth==True:\n",
        "        #     if target != 'J':\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'J'])]\n",
        "        #     else:\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'I'])]\n",
        "\n",
        "        #     self._data['stabf'] = self._data['stabf'].apply(lambda x: target if x == target else 'Z')\n",
        "        #     self._data['stabf'] = self._data['stabf'].map({target: 1, 'Z': 0}).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "        # # Random Undersampling\n",
        "        # X = self._data.drop('stabf', axis=1)\n",
        "        # y = self._data['stabf']\n",
        "\n",
        "        # # sampler = RandomUnderSampler(sampling_strategy='not minority', random_state=seed)\n",
        "        # # X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "\n",
        "        # # X_resampled['Class'] = y_resampled\n",
        "        # self._data = X\n",
        "\n",
        "        # The data is sorted by Class A,B,C the indexes of the dataframe have restarted by ignore index\n",
        "        self._data = self._data.sort_values(by=['stabf'], inplace=False,ignore_index = True)\n",
        "\n",
        "        # class_uniq contains the letters of the drivers A,B and it loops across all of them\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Find the total number of elements belonging to a class\n",
        "            tot_number=sum(self._data['stabf']==class_uniq)\n",
        "            # Number of elements to drop so that the class element is divisible by window size\n",
        "            to_drop=tot_number%window_size\n",
        "            # Returns the index of the first element of the class\n",
        "            index_to_start_removing=self._data[self._data['stabf']==class_uniq].index[0]\n",
        "            # Drop element from first element to the element required\n",
        "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
        "\n",
        "\n",
        "        # Resetting index of dataframe after dropping values\n",
        "        self._data = self._data.reset_index()\n",
        "        self._data = self._data.drop(['index'], axis=1)\n",
        "\n",
        "        index_starting_class=[] # This array contains the starting index of each class in the df\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Appending the index of first element of each clas\n",
        "            index_starting_class.append(self._data[self._data['stabf']==class_uniq].index[0])\n",
        "\n",
        "        # Create the sequence of indexs of the windows\n",
        "        sequences=[]\n",
        "        for i in range(len(index_starting_class)):\n",
        "            # Check if beginning of next class is there\n",
        "            if i!=len(index_starting_class)-1:\n",
        "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
        "            else:\n",
        "                ranges = np.arange(index_starting_class[i], len(self._data))\n",
        "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
        "                if len(ranges[j:j+self._window_size])==16:\n",
        "                    sequences.append(ranges[j:j+self._window_size])\n",
        "        self._sequences=sequences\n",
        "\n",
        "\n",
        "        # Take only the 'Class' which are the actual labels and store it in the labels of self\n",
        "        self._labels=self._data['stabf']\n",
        "        # Dropping columns which have constant measurements because they would return nan in std\n",
        "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
        "\n",
        "        # Function to normalize the data either with min_max or mean_std\n",
        "        if normalize and not auth:\n",
        "            for col in self._data.columns:\n",
        "                if normalize_method=='min_max':\n",
        "                    min_max_norm(self,col)\n",
        "                elif normalize_method==\"mean_std\":\n",
        "                    std_scaler(self,col)\n",
        "\n",
        "        # Create the array holding the windowed multidimensional arrays\n",
        "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
        "        y=[]\n",
        "\n",
        "        for n_row, sequence in enumerate(sequences):\n",
        "            X[n_row,:,:]=self._data.iloc[sequence]\n",
        "            # The corresponding driver of the sequence is the driver at first sequence\n",
        "            y.append(self._labels[sequence[0]])\n",
        "\n",
        "        assert len(y)==len(X)\n",
        "        # Assign the windowed dataset to the X of self\n",
        "        self._X= X\n",
        "\n",
        "        # Targets is a transformed version of y with drivers are encoded into 0 to 9\n",
        "        # targets = preprocessing.LabelEncoder().fit_transform(y)\n",
        "        # class_labels = encoder.classes_\n",
        "        # for code, label in enumerate(class_labels):\n",
        "        #   print(f'Code: {code} -> Label: {label}')\n",
        "        # targets = torch.as_tensor(targets)  # Just converting it to a pytorch tensor\n",
        "        self._y=y # Assign it to y of self\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._X)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs\n",
        "        labels = labels\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "\n",
        "def evaluateBinary(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            # loss = criterion(outputs, labels)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        # preds = (outputs > 0.5).float()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n"
      ],
      "metadata": {
        "id": "EjXMMgcXgo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/smart_grid_stability_augmented.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "df"
      ],
      "metadata": {
        "id": "WCTrkBiQnHgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title stabf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('stabf').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "VgTgy1u6Bmc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'marker' is a categorical column in your DataFrame\n",
        "encoder = LabelEncoder()\n",
        "df['stabf'] = encoder.fit_transform(df['stabf'])\n",
        "\n",
        "# Retrieve the mapping of numerical codes to original class labels\n",
        "class_labels = encoder.classes_\n",
        "\n",
        "# Display the mapping\n",
        "for code, label in enumerate(class_labels):\n",
        "    print(f'Code: {code} -> Label: {label}')\n",
        "df"
      ],
      "metadata": {
        "id": "rw0YcgjAPj9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create a new DataFrame containing only rows with value 0 in 'column_name'\n",
        "new_df = df[df['stabf'] == 0]\n",
        "\n",
        "# Display the new DataFrame\n",
        "new_df"
      ],
      "metadata": {
        "id": "a_8niT9pAhrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.to_csv('new_dataset.csv', index=False)\n",
        "new_df"
      ],
      "metadata": {
        "id": "8GBuKEhGAjIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "o723dQMYnDGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainSize = 0.9\n",
        "# valSize = 0.05\n",
        "a = CustomDataset()\n",
        "\n",
        "# Defining sizes\n",
        "train_size = int(trainSize * len(a))\n",
        "test_size = len(a)-train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    a, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=4,\n",
        "                                           shuffle=True,\n",
        "                                           drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "CUDeWckM9pOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (inputs,Labels) in enumerate(train_loader):\n",
        "  Labels = Labels.to(device)\n",
        "  print(Labels)"
      ],
      "metadata": {
        "id": "gCakxEj14FZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator and discriminator for the GAN\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, batch_size,window_size,num_features,latent_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_features = num_features\n",
        "        self.window_size = window_size\n",
        "       # self.fc1 = nn.Linear(latent_dim, 128)\n",
        "        self.fc1 = nn.Linear(num_features, latent_dim)\n",
        "        self.fc2 = nn.Linear(latent_dim, 128)\n",
        "        self.fc3 = nn.Linear(128,batch_size*window_size)\n",
        "        self.fc4 = nn.Linear(batch_size*window_size,num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        #return x.view(x.size(0), 1, -1)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 160)\n",
        "        self.fc2 = nn.Linear(160, 200)\n",
        "        self.fc3 = nn.Linear(200, 256)\n",
        "        self.fc4 = nn.Linear(256, 512)\n",
        "        self.fc5 = nn.Linear(512, 1)\n",
        "        self.window_size = window_size\n",
        "        self.num_features = num_features\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "      #x = x.view(-1, self.num_features)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      x = F.relu(self.fc4(x))\n",
        "      x = torch.sigmoid(self.fc5(x))\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "# Define the GAN training function\n",
        "\n",
        "def train_gan(generator, discriminator, train_loader, num_epochs=100, lr=0.0002,\n",
        "              device=torch.device('cpu')):\n",
        "\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    # Define the loss functions and optimizers\n",
        "\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (inputs,Labels) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)\n",
        "            Labels = Labels.to(device)\n",
        "\n",
        "            # Train discriminator on real data\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            # real_labels = torch.ones(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            real_labels = torch.zeros(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            real_outputs = discriminator(inputs)\n",
        "            discriminator_loss_real = adversarial_loss(real_outputs, real_labels)\n",
        "            discriminator_loss_real.backward()\n",
        "\n",
        "            # Train discriminator on fake data generated by the generator\n",
        "            generator_optimizer.zero_grad()\n",
        "            latent_inputs = torch.randn(inputs.shape[0], inputs.shape[1], inputs.shape[2]).to(device)\n",
        "            fake_inputs = generator(latent_inputs)\n",
        "            # fake_labels = torch.zeros(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            fake_labels = torch.ones(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            fake_outputs = discriminator(fake_inputs)\n",
        "            discriminator_loss_fake = adversarial_loss(fake_outputs, fake_labels)\n",
        "            discriminator_loss_fake.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "            # Train generator to generate samples that increase the discriminator loss\n",
        "            generator_optimizer.zero_grad()\n",
        "            latent_inputs = torch.randn(inputs.shape[0], inputs.shape[1], inputs.shape[2]).to(device)\n",
        "            fake_inputs = generator(latent_inputs)\n",
        "            # @S: generator wants to maximize the probability of the discriminator being wrong. So loss is computed between discriminator's output to a fake image and the\n",
        "            #fake label (if the fake label is 0)\n",
        "            # fake_labels = torch.zeros(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            fake_labels = torch.ones(inputs.size(0), inputs.size(1), 1).to(device)\n",
        "            generator_loss =adversarial_loss(discriminator(fake_inputs), real_labels)\n",
        "            generator_loss.backward()\n",
        "            generator_optimizer.step()\n",
        "            if i % 10 == 0:\n",
        "              print('Epoch [{}/{}], Step [{}/{}], Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'\n",
        "                      .format(epoch, num_epochs, i, len(train_loader),\n",
        "discriminator_loss_real.item() + discriminator_loss_fake.item(), generator_loss.item()))\n",
        "                # Return the trained generator\n",
        "    return generator,discriminator\n",
        "\n",
        "\n",
        "# Define the number of epochs and learning rate for training the GAN\n",
        "num_epochs = 250\n",
        "lr = 0.0002\n",
        "inputs, classes = next(iter(train_loader))\n",
        "# Create the GAN models\n",
        "batch_size, window_size, num_features = inputs.shape\n",
        "generator = Generator(batch_size,window_size,num_features)\n",
        "discriminator = Discriminator(batch_size, window_size, num_features)\n",
        "\n",
        "# Train the GAN on your data\n",
        "trained_generator_new,trained_discriminator = train_gan(generator, discriminator, train_loader, num_epochs=num_epochs, lr=lr)"
      ],
      "metadata": {
        "id": "PzRPx_6a972L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved state dictionaries\n",
        "trained_generator_new.load_state_dict(torch.load('/content/trained_generator_Grid (3).pth'))\n",
        "trained_discriminator.load_state_dict(torch.load('/content/trained_discriminator_Grid (3).pth'))"
      ],
      "metadata": {
        "id": "f_0HbNN3whbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store predicted class labels\n",
        "predicted_labels = []\n",
        "\n",
        "# Set the discriminator to evaluation mode\n",
        "trained_discriminator.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader:\n",
        "\n",
        "        # Get the discriminator's output for the current batch\n",
        "        inputs = inputs.to(device)\n",
        "        surrogate_outputs = trained_discriminator(inputs)\n",
        "\n",
        "        # Apply thresholding to determine class labels\n",
        "        threshold = 0.5\n",
        "        batch_predicted_labels = (surrogate_outputs >= threshold).int()  # 1 if >= threshold, 0 otherwise\n",
        "        predicted_labels.append(batch_predicted_labels)\n",
        "\n",
        "# Convert the list of predicted labels to a single tensor\n",
        "predicted_labels = torch.cat(predicted_labels, dim=0)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "id": "rjwXk3MradRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True labels (all 1)\n",
        "true_labels = torch.zeros_like(predicted_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = (predicted_labels == true_labels).sum().item()\n",
        "total_samples = true_labels.numel()  # Total number of elements in the tensor\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QzcjSa9wa1Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create a new DataFrame containing only rows with value 0 in 'column_name'\n",
        "new_df = df[df['stabf'] == 1]\n",
        "\n",
        "# Display the new DataFrame\n",
        "new_df"
      ],
      "metadata": {
        "id": "ccb8zbEfe0TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.to_csv('new_dataset.csv', index=False)\n",
        "new_df"
      ],
      "metadata": {
        "id": "qf2fXCLZe5T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainSize = 0.01\n",
        "# valSize = 0.05\n",
        "a = CustomDataset()\n",
        "\n",
        "# Defining sizes\n",
        "train_size = int(trainSize * len(a))\n",
        "test_size = len(a)-train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    a, [train_size, test_size])\n",
        "\n",
        "train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=4,\n",
        "                                           shuffle=True,\n",
        "                                           drop_last=True)\n",
        "\n",
        "test_loader1 = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "V5Y7DGqbe_x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (inputs,Labels) in enumerate(test_loader1):\n",
        "  Labels = Labels.to(device)\n",
        "  print(Labels)"
      ],
      "metadata": {
        "id": "jKhQwC4eHEFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store predicted class labels\n",
        "predicted_labels = []\n",
        "\n",
        "# Set the discriminator to evaluation mode\n",
        "trained_discriminator.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader1:\n",
        "\n",
        "        # Get the discriminator's output for the current batch\n",
        "        inputs = inputs.to(device)\n",
        "        surrogate_outputs = trained_discriminator(inputs)\n",
        "\n",
        "        # Apply thresholding to determine class labels\n",
        "        threshold = 0.5\n",
        "        batch_predicted_labels = (surrogate_outputs >= threshold).int()  # 1 if >= threshold, 0 otherwise\n",
        "        predicted_labels.append(batch_predicted_labels)\n",
        "\n",
        "# Convert the list of predicted labels to a single tensor\n",
        "predicted_labels = torch.cat(predicted_labels, dim=0)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "id": "hDTT-A3OfF0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True labels (all 1)\n",
        "true_labels = torch.ones_like(predicted_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = (predicted_labels == true_labels).sum().item()\n",
        "total_samples = true_labels.numel()  # Total number of elements in the tensor\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "l84VhMVDfRaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# List to store predicted class labels and true labels\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "# Set the discriminator to evaluation mode\n",
        "trained_discriminator.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Loop through the first test loader\n",
        "    for inputs, labels in test_loader1:\n",
        "        inputs = inputs.to(device)\n",
        "        surrogate_outputs = trained_discriminator(inputs)\n",
        "        threshold = 0.5\n",
        "        batch_predicted_labels = (surrogate_outputs >= threshold).int()\n",
        "        predicted_labels.append(batch_predicted_labels.cpu().numpy())\n",
        "        true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Loop through the second test loader\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        surrogate_outputs = trained_discriminator(inputs)\n",
        "        threshold = 0.5\n",
        "        batch_predicted_labels = (surrogate_outputs >= threshold).int()\n",
        "        predicted_labels.append(batch_predicted_labels.cpu().numpy())\n",
        "        true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Convert the list of predicted labels and true labels to numpy arrays\n",
        "predicted_labels = np.concatenate(predicted_labels)\n",
        "predicted_labels_selected = predicted_labels[:, 0, 0]\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels_selected)\n",
        "f1 = f1_score(true_labels, predicted_labels_selected)\n",
        "\n",
        "# Print accuracy and F1 score\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "CO2RI49qw6Zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}